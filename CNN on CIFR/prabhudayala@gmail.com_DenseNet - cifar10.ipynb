{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten, SeparableConv2D, Reshape\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "# import tensorflow as tf\n",
    "#tf.config.gpu.set_per_process_memory_fraction(0.75)\n",
    "#tf.config.gpu.set_per_process_memory_growth(True)\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "# k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "c1cea922-a38d-45da-f9d8-7977ab9c2dd2"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    "    #zoom_range=0.3\n",
    "    )\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(X_batch[i].astype(np.uint8))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    #flat = layers.Flatten()(AvgPooling)\n",
    "    #print(flat.shape)\n",
    "    output = layers.Conv2D(10,2,activation='softmax')(AvgPooling)\n",
    "    #print(output.shape)\n",
    "    output = Reshape((10,))(output)\n",
    "    #print(output.shape)\n",
    "    #model.add(Reshape((-1, 2, 2)))\n",
    "    #output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1120 19:54:55.745724 13960 deprecation.py:506] From C:\\Users\\user\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "num_filter = 30\n",
    "dropout_rate = 0.2\n",
    "l = 8\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/-W6y8xnd--U\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x186de3742b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://arxiv.org/pdf/1608.06993.pdf\n",
    "from IPython.display import IFrame, YouTubeVideo\n",
    "YouTubeVideo(id='-W6y8xnd--U', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9860
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "160abc05-9e09-4454-d453-0e33a7d95796",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 30)   810         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 30)   120         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 30)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 15)   4050        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32, 32, 15)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 45)   0           conv2d[0][0]                     \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 45)   180         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 45)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 15)   6075        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 15)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 60)   0           concatenate[0][0]                \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 60)   240         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 60)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 15)   8100        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 15)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 75)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 75)   300         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 75)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 15)   10125       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 15)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 90)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 90)   360         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 90)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 15)   12150       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 15)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 105)  0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 105)  420         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 105)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 15)   14175       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 15)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 120)  0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 120)  480         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 120)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 15)   16200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 15)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 135)  0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 135)  540         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 135)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 15)   18225       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 15)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 150)  0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 150)  600         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 150)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 15)   2250        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 15)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 15)   0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 15)   60          average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 15)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 15)   2025        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16, 16, 15)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 30)   0           average_pooling2d[0][0]          \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 30)   120         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 30)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 15)   4050        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 16, 15)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 45)   0           concatenate_8[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 45)   180         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 45)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 15)   6075        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 16, 15)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 16, 16, 60)   0           concatenate_9[0][0]              \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 60)   240         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 60)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 15)   8100        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 16, 15)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 75)   0           concatenate_10[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 75)   300         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 75)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 15)   10125       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 16, 16, 15)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 90)   0           concatenate_11[0][0]             \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 90)   360         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 90)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 15)   12150       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 16, 15)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 105)  0           concatenate_12[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 105)  420         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 105)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 15)   14175       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16, 16, 15)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 120)  0           concatenate_13[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 120)  480         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 120)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 15)   16200       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 15)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 135)  0           concatenate_14[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 135)  540         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 135)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 15)   2025        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 15)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 15)     0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 15)     60          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 15)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 15)     2025        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 8, 8, 15)     0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 8, 8, 30)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 30)     120         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 30)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 15)     4050        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 8, 8, 15)     0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 8, 8, 45)     0           concatenate_16[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 45)     180         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 45)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 15)     6075        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 8, 8, 15)     0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8, 60)     0           concatenate_17[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 60)     240         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 60)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 15)     8100        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 8, 8, 15)     0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 8, 8, 75)     0           concatenate_18[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 75)     300         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 75)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 15)     10125       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8, 8, 15)     0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 90)     0           concatenate_19[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 90)     360         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 90)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 15)     12150       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 8, 8, 15)     0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 105)    0           concatenate_20[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 105)    420         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 105)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 15)     14175       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 8, 8, 15)     0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 8, 8, 120)    0           concatenate_21[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 120)    480         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 120)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 15)     16200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 8, 8, 15)     0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 8, 8, 135)    0           concatenate_22[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 135)    540         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 135)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 15)     2025        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 8, 8, 15)     0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 15)     0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 15)     60          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 15)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 4, 4, 15)     2025        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 4, 4, 15)     0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 4, 4, 30)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 30)     120         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 30)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 4, 15)     4050        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 4, 4, 15)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 4, 4, 45)     0           concatenate_24[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 45)     180         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 45)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 15)     6075        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 4, 4, 15)     0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 4, 4, 60)     0           concatenate_25[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 60)     240         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 60)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 15)     8100        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 4, 4, 15)     0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 4, 4, 75)     0           concatenate_26[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 75)     300         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 75)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 15)     10125       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 4, 4, 15)     0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 4, 4, 90)     0           concatenate_27[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 90)     360         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 90)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 15)     12150       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 4, 4, 15)     0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 4, 4, 105)    0           concatenate_28[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 105)    420         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 105)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 15)     14175       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 4, 4, 15)     0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 4, 4, 120)    0           concatenate_29[0][0]             \n",
      "                                                                 dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 4, 120)    480         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 120)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 15)     16200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 4, 4, 15)     0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 4, 4, 135)    0           concatenate_30[0][0]             \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4, 4, 135)    540         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 135)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 2, 2, 135)    0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 1, 1, 10)     5410        average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 10)           0           conv2d_36[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 331,660\n",
      "Trainable params: 325,990\n",
      "Non-trainable params: 5,670\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1771
    },
    "colab_type": "code",
    "id": "crhGk7kEhXAz",
    "outputId": "e3e2d0d0-1492-41ab-df5b-5a7ecd705c2c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1120 19:55:00.599825 13960 deprecation.py:323] From C:\\Users\\user\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 41s 823us/sample - loss: 1.5205 - acc: 0.4386 - val_loss: 1.3896 - val_acc: 0.4965\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 36s 717us/sample - loss: 1.0982 - acc: 0.6048 - val_loss: 1.1493 - val_acc: 0.6072\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 36s 717us/sample - loss: 0.9201 - acc: 0.6736 - val_loss: 1.3172 - val_acc: 0.5754\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 36s 718us/sample - loss: 0.8156 - acc: 0.7097 - val_loss: 0.9007 - val_acc: 0.7041\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 36s 718us/sample - loss: 0.7434 - acc: 0.7369 - val_loss: 0.7686 - val_acc: 0.7362\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 36s 719us/sample - loss: 0.6893 - acc: 0.7572 - val_loss: 1.3850 - val_acc: 0.6102\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 36s 721us/sample - loss: 0.6432 - acc: 0.7747 - val_loss: 0.7889 - val_acc: 0.7493\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 36s 720us/sample - loss: 0.6053 - acc: 0.7868 - val_loss: 0.7597 - val_acc: 0.7591\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 36s 720us/sample - loss: 0.5751 - acc: 0.7984 - val_loss: 0.8693 - val_acc: 0.7273\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 36s 726us/sample - loss: 0.5535 - acc: 0.8058 - val_loss: 0.8518 - val_acc: 0.7311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x186e83cfd68>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "a0345aa5-79ff-4e56-eb94-50437b43c4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.8518 - acc: 0.7311\n",
      "Test loss: 0.8518377923488617\n",
      "Test accuracy: 0.7311\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UE3lF6EH1r_L",
    "outputId": "92df862c-76a7-4a02-9533-6c164bc5984d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filter = 30\n",
    "dropout_rate = 0.2\n",
    "l = 8\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9860
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "160abc05-9e09-4454-d453-0e33a7d95796",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 30)   810         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 30)   120         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 30)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 15)   4050        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 32, 32, 15)   0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 32, 32, 45)   0           conv2d_37[0][0]                  \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 45)   180         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 45)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 15)   6075        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 32, 32, 15)   0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 32, 32, 60)   0           concatenate_32[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 60)   240         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 60)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 32, 32, 15)   8100        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 32, 32, 15)   0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 32, 32, 75)   0           concatenate_33[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 75)   300         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 75)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 32, 32, 15)   10125       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 32, 32, 15)   0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 32, 32, 90)   0           concatenate_34[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 90)   360         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 90)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 32, 15)   12150       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 32, 32, 15)   0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 32, 32, 105)  0           concatenate_35[0][0]             \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 105)  420         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 32, 32, 105)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 32, 15)   14175       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 32, 32, 15)   0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 32, 32, 120)  0           concatenate_36[0][0]             \n",
      "                                                                 dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 32, 120)  480         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 32, 32, 120)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 32, 32, 15)   16200       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 32, 32, 15)   0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 32, 32, 135)  0           concatenate_37[0][0]             \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 32, 32, 135)  540         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 32, 32, 135)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 32, 32, 15)   18225       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 32, 32, 15)   0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 32, 32, 150)  0           concatenate_38[0][0]             \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 32, 32, 150)  600         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 32, 32, 150)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 32, 32, 15)   2250        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 32, 32, 15)   0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 16, 16, 15)   0           dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16, 16, 15)   60          average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16, 16, 15)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 16, 16, 15)   2025        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 16, 16, 15)   0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 16, 16, 30)   0           average_pooling2d_4[0][0]        \n",
      "                                                                 dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 16, 16, 30)   120         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 16, 16, 30)   0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 16, 16, 15)   4050        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 16, 16, 15)   0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 16, 16, 45)   0           concatenate_40[0][0]             \n",
      "                                                                 dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 16, 16, 45)   180         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 16, 45)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 16, 16, 15)   6075        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 16, 16, 15)   0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 16, 16, 60)   0           concatenate_41[0][0]             \n",
      "                                                                 dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 16, 16, 60)   240         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16, 16, 60)   0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 16, 16, 15)   8100        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 16, 16, 15)   0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 16, 16, 75)   0           concatenate_42[0][0]             \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 16, 16, 75)   300         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 16, 16, 75)   0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 16, 16, 15)   10125       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 16, 16, 15)   0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 16, 16, 90)   0           concatenate_43[0][0]             \n",
      "                                                                 dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 16, 16, 90)   360         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 16, 16, 90)   0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 16, 16, 15)   12150       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 16, 16, 15)   0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 16, 16, 105)  0           concatenate_44[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 16, 16, 105)  420         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16, 16, 105)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 16, 16, 15)   14175       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 16, 16, 15)   0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 16, 16, 120)  0           concatenate_45[0][0]             \n",
      "                                                                 dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 16, 16, 120)  480         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 16, 16, 120)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 16, 16, 15)   16200       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 16, 16, 15)   0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 16, 16, 135)  0           concatenate_46[0][0]             \n",
      "                                                                 dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 16, 16, 135)  540         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 16, 16, 135)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 15)   2025        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 16, 16, 15)   0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 8, 8, 15)     0           dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 8, 8, 15)     60          average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 8, 8, 15)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 8, 15)     2025        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 8, 8, 15)     0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 8, 8, 30)     0           average_pooling2d_5[0][0]        \n",
      "                                                                 dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 8, 8, 30)     120         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 8, 8, 30)     0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 8, 8, 15)     4050        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 8, 8, 15)     0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 8, 8, 45)     0           concatenate_48[0][0]             \n",
      "                                                                 dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 8, 8, 45)     180         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 8, 8, 45)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 8, 8, 15)     6075        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 8, 8, 15)     0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 8, 8, 60)     0           concatenate_49[0][0]             \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 8, 8, 60)     240         concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 8, 8, 60)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 8, 8, 15)     8100        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 8, 8, 15)     0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 8, 8, 75)     0           concatenate_50[0][0]             \n",
      "                                                                 dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 8, 8, 75)     300         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 8, 8, 75)     0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 8, 8, 15)     10125       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 8, 8, 15)     0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 8, 8, 90)     0           concatenate_51[0][0]             \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 8, 8, 90)     360         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 8, 8, 90)     0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 8, 8, 15)     12150       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 8, 8, 15)     0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 8, 8, 105)    0           concatenate_52[0][0]             \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 8, 8, 105)    420         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 8, 8, 105)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 8, 8, 15)     14175       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 8, 8, 15)     0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 8, 8, 120)    0           concatenate_53[0][0]             \n",
      "                                                                 dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 8, 8, 120)    480         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 8, 8, 120)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 8, 8, 15)     16200       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 8, 8, 15)     0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 8, 8, 135)    0           concatenate_54[0][0]             \n",
      "                                                                 dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 8, 8, 135)    540         concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 8, 135)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 8, 8, 15)     2025        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 8, 8, 15)     0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 4, 4, 15)     0           dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 4, 4, 15)     60          average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 4, 4, 15)     0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 4, 4, 15)     2025        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 4, 4, 15)     0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 4, 4, 30)     0           average_pooling2d_6[0][0]        \n",
      "                                                                 dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 4, 4, 30)     120         concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 4, 4, 30)     0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 4, 4, 15)     4050        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 4, 4, 15)     0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 4, 4, 45)     0           concatenate_56[0][0]             \n",
      "                                                                 dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 4, 4, 45)     180         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 4, 4, 45)     0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 4, 4, 15)     6075        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 4, 4, 15)     0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 4, 4, 60)     0           concatenate_57[0][0]             \n",
      "                                                                 dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 4, 4, 60)     240         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 4, 4, 60)     0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 4, 4, 15)     8100        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 4, 4, 15)     0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 4, 4, 75)     0           concatenate_58[0][0]             \n",
      "                                                                 dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 4, 4, 75)     300         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 4, 4, 75)     0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 4, 4, 15)     10125       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 4, 4, 15)     0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 4, 4, 90)     0           concatenate_59[0][0]             \n",
      "                                                                 dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 4, 4, 90)     360         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 4, 4, 90)     0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 4, 4, 15)     12150       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 4, 4, 15)     0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 4, 4, 105)    0           concatenate_60[0][0]             \n",
      "                                                                 dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 4, 4, 105)    420         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 4, 4, 105)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 4, 4, 15)     14175       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 4, 4, 15)     0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 4, 4, 120)    0           concatenate_61[0][0]             \n",
      "                                                                 dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 4, 4, 120)    480         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 4, 4, 120)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 4, 4, 15)     16200       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 4, 4, 15)     0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 4, 4, 135)    0           concatenate_62[0][0]             \n",
      "                                                                 dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 4, 4, 135)    540         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 4, 4, 135)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 2, 2, 135)    0           activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 1, 1, 10)     5410        average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 10)           0           conv2d_73[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 331,660\n",
      "Trainable params: 325,990\n",
      "Non-trainable params: 5,670\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Model(inputs=[input], outputs=[output])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from tf.keras.callbacks import ModelCheckpoint\n",
    "filepath=\"DNST_model_with_augmentation.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 1.5828 - acc: 0.4118\n",
      "Epoch 00001: val_acc improved from -inf to 0.46860, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 60s 154ms/step - loss: 1.5822 - acc: 0.4121 - val_loss: 1.6314 - val_acc: 0.4686\n",
      "Epoch 2/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 1.2251 - acc: 0.5543\n",
      "Epoch 00002: val_acc improved from 0.46860 to 0.52820, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 1.2252 - acc: 0.5544 - val_loss: 1.4968 - val_acc: 0.5282\n",
      "Epoch 3/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 1.0731 - acc: 0.6153\n",
      "Epoch 00003: val_acc improved from 0.52820 to 0.55770, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 1.0732 - acc: 0.6153 - val_loss: 1.5441 - val_acc: 0.5577\n",
      "Epoch 4/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.9887 - acc: 0.6472\n",
      "Epoch 00004: val_acc improved from 0.55770 to 0.60600, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.9890 - acc: 0.6471 - val_loss: 1.2903 - val_acc: 0.6060\n",
      "Epoch 5/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.9199 - acc: 0.6712\n",
      "Epoch 00005: val_acc did not improve from 0.60600\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.9198 - acc: 0.6712 - val_loss: 1.5100 - val_acc: 0.5800\n",
      "Epoch 6/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.6889\n",
      "Epoch 00006: val_acc improved from 0.60600 to 0.69620, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.8741 - acc: 0.6890 - val_loss: 0.9544 - val_acc: 0.6962\n",
      "Epoch 7/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.8238 - acc: 0.7086\n",
      "Epoch 00007: val_acc did not improve from 0.69620\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.8233 - acc: 0.7087 - val_loss: 1.0003 - val_acc: 0.6700\n",
      "Epoch 8/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.7861 - acc: 0.7215\n",
      "Epoch 00008: val_acc improved from 0.69620 to 0.73200, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.7859 - acc: 0.7216 - val_loss: 0.8216 - val_acc: 0.7320\n",
      "Epoch 9/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.7478 - acc: 0.7353\n",
      "Epoch 00009: val_acc did not improve from 0.73200\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.7479 - acc: 0.7354 - val_loss: 1.2476 - val_acc: 0.6389\n",
      "Epoch 10/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.7498\n",
      "Epoch 00010: val_acc did not improve from 0.73200\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.7114 - acc: 0.7499 - val_loss: 0.8482 - val_acc: 0.7251\n",
      "Epoch 11/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.7547\n",
      "Epoch 00011: val_acc did not improve from 0.73200\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.6951 - acc: 0.7546 - val_loss: 0.9972 - val_acc: 0.7095\n",
      "Epoch 12/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.6679 - acc: 0.7642- ET\n",
      "Epoch 00012: val_acc improved from 0.73200 to 0.75670, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.6683 - acc: 0.7641 - val_loss: 0.7583 - val_acc: 0.7567\n",
      "Epoch 13/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.6521 - acc: 0.7722\n",
      "Epoch 00013: val_acc did not improve from 0.75670\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.6522 - acc: 0.7722 - val_loss: 0.8058 - val_acc: 0.7509\n",
      "Epoch 14/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.6283 - acc: 0.7802\n",
      "Epoch 00014: val_acc improved from 0.75670 to 0.76450, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.6284 - acc: 0.7801 - val_loss: 0.7538 - val_acc: 0.7645\n",
      "Epoch 15/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.6174 - acc: 0.7837\n",
      "Epoch 00015: val_acc did not improve from 0.76450\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.6176 - acc: 0.7837 - val_loss: 0.7842 - val_acc: 0.7588\n",
      "Epoch 16/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.6084 - acc: 0.7854- ETA:  - ETA: 1s - loss: \n",
      "Epoch 00016: val_acc improved from 0.76450 to 0.78440, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.6085 - acc: 0.7854 - val_loss: 0.6871 - val_acc: 0.7844\n",
      "Epoch 17/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.7942\n",
      "Epoch 00017: val_acc did not improve from 0.78440\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.5902 - acc: 0.7942 - val_loss: 0.9883 - val_acc: 0.7219\n",
      "Epoch 18/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8008- ETA: 2s - lo\n",
      "Epoch 00018: val_acc improved from 0.78440 to 0.79370, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.5736 - acc: 0.8007 - val_loss: 0.6574 - val_acc: 0.7937\n",
      "Epoch 19/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5681 - acc: 0.8007\n",
      "Epoch 00019: val_acc did not improve from 0.79370\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.5678 - acc: 0.8009 - val_loss: 0.7524 - val_acc: 0.7711\n",
      "Epoch 20/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.8068\n",
      "Epoch 00020: val_acc did not improve from 0.79370\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.5496 - acc: 0.8068 - val_loss: 0.6788 - val_acc: 0.7903\n",
      "Epoch 21/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.8102\n",
      "Epoch 00021: val_acc improved from 0.79370 to 0.80030, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.5402 - acc: 0.8102 - val_loss: 0.6534 - val_acc: 0.8003\n",
      "Epoch 22/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.8141\n",
      "Epoch 00022: val_acc did not improve from 0.80030\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.5350 - acc: 0.8142 - val_loss: 0.8933 - val_acc: 0.7418\n",
      "Epoch 23/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.8182\n",
      "Epoch 00023: val_acc improved from 0.80030 to 0.81440, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.5238 - acc: 0.8181 - val_loss: 0.6070 - val_acc: 0.8144\n",
      "Epoch 24/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5135 - acc: 0.8205\n",
      "Epoch 00024: val_acc did not improve from 0.81440\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.5138 - acc: 0.8204 - val_loss: 0.7269 - val_acc: 0.7846\n",
      "Epoch 25/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5109 - acc: 0.8221\n",
      "Epoch 00025: val_acc did not improve from 0.81440\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.5108 - acc: 0.8222 - val_loss: 0.8926 - val_acc: 0.7431\n",
      "Epoch 26/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8254\n",
      "Epoch 00026: val_acc did not improve from 0.81440\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.5018 - acc: 0.8256 - val_loss: 0.6991 - val_acc: 0.7883\n",
      "Epoch 27/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.8255- ETA: 1s - loss: 0.4952 -\n",
      "Epoch 00027: val_acc did not improve from 0.81440\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4955 - acc: 0.8254 - val_loss: 0.7165 - val_acc: 0.7896\n",
      "Epoch 28/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.8299\n",
      "Epoch 00028: val_acc did not improve from 0.81440\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4909 - acc: 0.8299 - val_loss: 0.9951 - val_acc: 0.7261\n",
      "Epoch 29/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4791 - acc: 0.8312- ETA: 3s - loss: 0. - ETA: 1s - loss: 0.\n",
      "Epoch 00029: val_acc did not improve from 0.81440\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4790 - acc: 0.8313 - val_loss: 1.0440 - val_acc: 0.7227\n",
      "Epoch 30/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.8314\n",
      "Epoch 00030: val_acc did not improve from 0.81440\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4796 - acc: 0.8314 - val_loss: 0.6536 - val_acc: 0.8104\n",
      "Epoch 31/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.8359- \n",
      "Epoch 00031: val_acc did not improve from 0.81440\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4723 - acc: 0.8358 - val_loss: 0.6199 - val_acc: 0.8133\n",
      "Epoch 32/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.8388\n",
      "Epoch 00032: val_acc improved from 0.81440 to 0.82650, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.4633 - acc: 0.8387 - val_loss: 0.5701 - val_acc: 0.8265\n",
      "Epoch 33/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.8401\n",
      "Epoch 00033: val_acc did not improve from 0.82650\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4575 - acc: 0.8401 - val_loss: 0.6803 - val_acc: 0.7946\n",
      "Epoch 34/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8420\n",
      "Epoch 00034: val_acc did not improve from 0.82650\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4523 - acc: 0.8420 - val_loss: 0.5546 - val_acc: 0.8257\n",
      "Epoch 35/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.8452\n",
      "Epoch 00035: val_acc did not improve from 0.82650\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4450 - acc: 0.8452 - val_loss: 0.6944 - val_acc: 0.8068\n",
      "Epoch 36/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4479 - acc: 0.8435\n",
      "Epoch 00036: val_acc did not improve from 0.82650\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4478 - acc: 0.8435 - val_loss: 0.6274 - val_acc: 0.8121\n",
      "Epoch 37/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8473\n",
      "Epoch 00037: val_acc did not improve from 0.82650\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.4397 - acc: 0.8473 - val_loss: 0.6816 - val_acc: 0.7983\n",
      "Epoch 38/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.8484\n",
      "Epoch 00038: val_acc improved from 0.82650 to 0.83090, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.4366 - acc: 0.8484 - val_loss: 0.5811 - val_acc: 0.8309\n",
      "Epoch 39/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8501\n",
      "Epoch 00039: val_acc did not improve from 0.83090\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4284 - acc: 0.8502 - val_loss: 0.6725 - val_acc: 0.8038\n",
      "Epoch 40/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.8513\n",
      "Epoch 00040: val_acc did not improve from 0.83090\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4278 - acc: 0.8512 - val_loss: 0.8227 - val_acc: 0.7797\n",
      "Epoch 41/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8532\n",
      "Epoch 00041: val_acc did not improve from 0.83090\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4224 - acc: 0.8532 - val_loss: 0.6231 - val_acc: 0.8168\n",
      "Epoch 42/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8539\n",
      "Epoch 00042: val_acc did not improve from 0.83090\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4172 - acc: 0.8539 - val_loss: 0.6186 - val_acc: 0.8260\n",
      "Epoch 43/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8545\n",
      "Epoch 00043: val_acc improved from 0.83090 to 0.85240, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4170 - acc: 0.8545 - val_loss: 0.4899 - val_acc: 0.8524\n",
      "Epoch 44/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8559- ETA: 1s - loss: 0.412\n",
      "Epoch 00044: val_acc did not improve from 0.85240\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4128 - acc: 0.8559 - val_loss: 0.5086 - val_acc: 0.8433\n",
      "Epoch 45/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8587\n",
      "Epoch 00045: val_acc did not improve from 0.85240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4082 - acc: 0.8586 - val_loss: 0.5788 - val_acc: 0.8274\n",
      "Epoch 46/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8578\n",
      "Epoch 00046: val_acc did not improve from 0.85240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4061 - acc: 0.8578 - val_loss: 0.6601 - val_acc: 0.8161\n",
      "Epoch 47/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8595\n",
      "Epoch 00047: val_acc did not improve from 0.85240\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.4040 - acc: 0.8596 - val_loss: 0.5143 - val_acc: 0.8487\n",
      "Epoch 48/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8602\n",
      "Epoch 00048: val_acc improved from 0.85240 to 0.85550, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.4007 - acc: 0.8601 - val_loss: 0.4669 - val_acc: 0.8555\n",
      "Epoch 49/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3964 - acc: 0.8624\n",
      "Epoch 00049: val_acc did not improve from 0.85550\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3966 - acc: 0.8623 - val_loss: 0.4915 - val_acc: 0.8514\n",
      "Epoch 50/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8623\n",
      "Epoch 00050: val_acc did not improve from 0.85550\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.3940 - acc: 0.8623 - val_loss: 0.5701 - val_acc: 0.8324\n",
      "Epoch 51/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8635\n",
      "Epoch 00051: val_acc did not improve from 0.85550\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3902 - acc: 0.8635 - val_loss: 0.5139 - val_acc: 0.8461\n",
      "Epoch 52/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8642\n",
      "Epoch 00052: val_acc improved from 0.85550 to 0.85950, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3891 - acc: 0.8639 - val_loss: 0.4445 - val_acc: 0.8595\n",
      "Epoch 53/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8647\n",
      "Epoch 00053: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3876 - acc: 0.8647 - val_loss: 0.4687 - val_acc: 0.8546\n",
      "Epoch 54/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8660\n",
      "Epoch 00054: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3845 - acc: 0.8661 - val_loss: 0.6479 - val_acc: 0.8074\n",
      "Epoch 55/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8688\n",
      "Epoch 00055: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3761 - acc: 0.8688 - val_loss: 0.5229 - val_acc: 0.8410\n",
      "Epoch 56/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8675\n",
      "Epoch 00056: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3773 - acc: 0.8676 - val_loss: 0.5558 - val_acc: 0.8400\n",
      "Epoch 57/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3695 - acc: 0.8704\n",
      "Epoch 00057: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3699 - acc: 0.8702 - val_loss: 0.4858 - val_acc: 0.8547\n",
      "Epoch 58/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8675\n",
      "Epoch 00058: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.3745 - acc: 0.8675 - val_loss: 0.5093 - val_acc: 0.8520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8732\n",
      "Epoch 00059: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.3638 - acc: 0.8732 - val_loss: 0.5298 - val_acc: 0.8477\n",
      "Epoch 60/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8731\n",
      "Epoch 00060: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3616 - acc: 0.8731 - val_loss: 0.5645 - val_acc: 0.8332\n",
      "Epoch 61/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8717\n",
      "Epoch 00061: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3659 - acc: 0.8717 - val_loss: 0.5021 - val_acc: 0.8563\n",
      "Epoch 62/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8747\n",
      "Epoch 00062: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3590 - acc: 0.8748 - val_loss: 0.5306 - val_acc: 0.8446\n",
      "Epoch 63/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8754\n",
      "Epoch 00063: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3569 - acc: 0.8754 - val_loss: 0.4908 - val_acc: 0.8561\n",
      "Epoch 64/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8757\n",
      "Epoch 00064: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3596 - acc: 0.8757 - val_loss: 0.5622 - val_acc: 0.8426\n",
      "Epoch 65/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8758\n",
      "Epoch 00065: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3538 - acc: 0.8758 - val_loss: 0.4931 - val_acc: 0.8501\n",
      "Epoch 66/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8752\n",
      "Epoch 00066: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3515 - acc: 0.8753 - val_loss: 0.6218 - val_acc: 0.8253\n",
      "Epoch 67/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8768\n",
      "Epoch 00067: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3524 - acc: 0.8768 - val_loss: 0.5253 - val_acc: 0.8540\n",
      "Epoch 68/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8777\n",
      "Epoch 00068: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.3480 - acc: 0.8778 - val_loss: 0.4886 - val_acc: 0.8572\n",
      "Epoch 69/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8779\n",
      "Epoch 00069: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3497 - acc: 0.8778 - val_loss: 0.5784 - val_acc: 0.8344\n",
      "Epoch 70/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8783\n",
      "Epoch 00070: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3485 - acc: 0.8783 - val_loss: 0.5781 - val_acc: 0.8340\n",
      "Epoch 71/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8808\n",
      "Epoch 00071: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3404 - acc: 0.8808 - val_loss: 0.6942 - val_acc: 0.8069\n",
      "Epoch 72/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8813\n",
      "Epoch 00072: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3416 - acc: 0.8812 - val_loss: 0.5497 - val_acc: 0.8478\n",
      "Epoch 73/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8818\n",
      "Epoch 00073: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3362 - acc: 0.8818 - val_loss: 0.5681 - val_acc: 0.8365\n",
      "Epoch 74/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8789\n",
      "Epoch 00074: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3389 - acc: 0.8789 - val_loss: 0.5586 - val_acc: 0.8362\n",
      "Epoch 75/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8819\n",
      "Epoch 00075: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3348 - acc: 0.8819 - val_loss: 0.5306 - val_acc: 0.8500\n",
      "Epoch 76/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8811\n",
      "Epoch 00076: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3355 - acc: 0.8810 - val_loss: 0.4782 - val_acc: 0.8595\n",
      "Epoch 77/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8827\n",
      "Epoch 00077: val_acc did not improve from 0.85950\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3381 - acc: 0.8827 - val_loss: 0.5151 - val_acc: 0.8524\n",
      "Epoch 78/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8815\n",
      "Epoch 00078: val_acc improved from 0.85950 to 0.87430, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3318 - acc: 0.8815 - val_loss: 0.4415 - val_acc: 0.8743\n",
      "Epoch 79/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8840\n",
      "Epoch 00079: val_acc did not improve from 0.87430\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3330 - acc: 0.8839 - val_loss: 0.4590 - val_acc: 0.8625\n",
      "Epoch 80/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.8833\n",
      "Epoch 00080: val_acc did not improve from 0.87430\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3343 - acc: 0.8831 - val_loss: 0.4768 - val_acc: 0.8615\n",
      "Epoch 81/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.8884\n",
      "Epoch 00081: val_acc did not improve from 0.87430\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3216 - acc: 0.8883 - val_loss: 0.6608 - val_acc: 0.8208\n",
      "Epoch 82/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.8855\n",
      "Epoch 00082: val_acc did not improve from 0.87430\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3248 - acc: 0.8854 - val_loss: 0.4598 - val_acc: 0.8683\n",
      "Epoch 83/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8869\n",
      "Epoch 00083: val_acc did not improve from 0.87430\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3252 - acc: 0.8869 - val_loss: 0.5641 - val_acc: 0.8395\n",
      "Epoch 84/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.8889\n",
      "Epoch 00084: val_acc did not improve from 0.87430\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3202 - acc: 0.8889 - val_loss: 0.5420 - val_acc: 0.8431\n",
      "Epoch 85/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.8859\n",
      "Epoch 00085: val_acc did not improve from 0.87430\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3238 - acc: 0.8859 - val_loss: 0.5038 - val_acc: 0.8565\n",
      "Epoch 86/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.8879\n",
      "Epoch 00086: val_acc did not improve from 0.87430\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3194 - acc: 0.8879 - val_loss: 0.5657 - val_acc: 0.8461\n",
      "Epoch 87/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.8852\n",
      "Epoch 00087: val_acc improved from 0.87430 to 0.87660, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.3225 - acc: 0.8853 - val_loss: 0.4154 - val_acc: 0.8766\n",
      "Epoch 88/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.8910\n",
      "Epoch 00088: val_acc improved from 0.87660 to 0.87770, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.3152 - acc: 0.8910 - val_loss: 0.4189 - val_acc: 0.8777\n",
      "Epoch 89/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.8897\n",
      "Epoch 00089: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.3168 - acc: 0.8897 - val_loss: 0.5406 - val_acc: 0.8467\n",
      "Epoch 90/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.8919\n",
      "Epoch 00090: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.3143 - acc: 0.8918 - val_loss: 0.4971 - val_acc: 0.8576\n",
      "Epoch 91/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8897\n",
      "Epoch 00091: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3147 - acc: 0.8897 - val_loss: 0.5005 - val_acc: 0.8551\n",
      "Epoch 92/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8930\n",
      "Epoch 00092: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3110 - acc: 0.8929 - val_loss: 0.5630 - val_acc: 0.8422\n",
      "Epoch 93/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.8926\n",
      "Epoch 00093: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3087 - acc: 0.8927 - val_loss: 0.4311 - val_acc: 0.8731\n",
      "Epoch 94/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.8917\n",
      "Epoch 00094: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3081 - acc: 0.8916 - val_loss: 0.4066 - val_acc: 0.8756\n",
      "Epoch 95/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.8905\n",
      "Epoch 00095: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.3087 - acc: 0.8906 - val_loss: 0.6620 - val_acc: 0.8215\n",
      "Epoch 96/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.8906- ETA: 1s - loss:\n",
      "Epoch 00096: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.3117 - acc: 0.8906 - val_loss: 0.5373 - val_acc: 0.8440\n",
      "Epoch 97/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.8945- ETA: 1s - loss: \n",
      "Epoch 00097: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3057 - acc: 0.8944 - val_loss: 0.4636 - val_acc: 0.8656\n",
      "Epoch 98/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.8942\n",
      "Epoch 00098: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3019 - acc: 0.8943 - val_loss: 0.4248 - val_acc: 0.8727\n",
      "Epoch 99/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8936- ETA\n",
      "Epoch 00099: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3045 - acc: 0.8936 - val_loss: 0.6297 - val_acc: 0.8224\n",
      "Epoch 100/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.8938\n",
      "Epoch 00100: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3028 - acc: 0.8936 - val_loss: 0.5426 - val_acc: 0.8517\n",
      "Epoch 101/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.8951\n",
      "Epoch 00101: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3008 - acc: 0.8952 - val_loss: 0.5313 - val_acc: 0.8543\n",
      "Epoch 102/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.8945\n",
      "Epoch 00102: val_acc did not improve from 0.87770\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3011 - acc: 0.8945 - val_loss: 0.4794 - val_acc: 0.8612\n",
      "Epoch 103/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.8942\n",
      "Epoch 00103: val_acc improved from 0.87770 to 0.88230, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.3006 - acc: 0.8942 - val_loss: 0.3959 - val_acc: 0.8823\n",
      "Epoch 104/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.8962\n",
      "Epoch 00104: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2971 - acc: 0.8961 - val_loss: 0.4067 - val_acc: 0.8777\n",
      "Epoch 105/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.8986\n",
      "Epoch 00105: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2936 - acc: 0.8986 - val_loss: 0.4316 - val_acc: 0.8718\n",
      "Epoch 106/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.8981\n",
      "Epoch 00106: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2920 - acc: 0.8982 - val_loss: 0.4784 - val_acc: 0.8633\n",
      "Epoch 107/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.8959\n",
      "Epoch 00107: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2937 - acc: 0.8959 - val_loss: 0.4591 - val_acc: 0.8692\n",
      "Epoch 108/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2972 - acc: 0.8963\n",
      "Epoch 00108: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2973 - acc: 0.8962 - val_loss: 0.4202 - val_acc: 0.8818\n",
      "Epoch 109/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.8960\n",
      "Epoch 00109: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2943 - acc: 0.8960 - val_loss: 0.6542 - val_acc: 0.8255\n",
      "Epoch 110/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.8978\n",
      "Epoch 00110: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2944 - acc: 0.8978 - val_loss: 0.5613 - val_acc: 0.8494\n",
      "Epoch 111/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.8971\n",
      "Epoch 00111: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2909 - acc: 0.8970 - val_loss: 0.5055 - val_acc: 0.8578\n",
      "Epoch 112/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.8979\n",
      "Epoch 00112: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2894 - acc: 0.8978 - val_loss: 0.4560 - val_acc: 0.8671\n",
      "Epoch 113/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.8980\n",
      "Epoch 00113: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2882 - acc: 0.8980 - val_loss: 0.4216 - val_acc: 0.8762\n",
      "Epoch 114/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.8981- ETA: 5s - loss: 0 - E\n",
      "Epoch 00114: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2893 - acc: 0.8980 - val_loss: 0.4197 - val_acc: 0.8782\n",
      "Epoch 115/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.8992\n",
      "Epoch 00115: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2886 - acc: 0.8993 - val_loss: 0.3861 - val_acc: 0.8812\n",
      "Epoch 116/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.8998\n",
      "Epoch 00116: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 37s 96ms/step - loss: 0.2863 - acc: 0.8997 - val_loss: 0.5531 - val_acc: 0.8425\n",
      "Epoch 117/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9008- ETA: 1s - loss: 0.2842\n",
      "Epoch 00117: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2854 - acc: 0.9009 - val_loss: 0.4595 - val_acc: 0.8674\n",
      "Epoch 118/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9030\n",
      "Epoch 00118: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2816 - acc: 0.9029 - val_loss: 0.3942 - val_acc: 0.8820\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.8984\n",
      "Epoch 00119: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2854 - acc: 0.8985 - val_loss: 0.4338 - val_acc: 0.8763\n",
      "Epoch 120/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9026\n",
      "Epoch 00120: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2811 - acc: 0.9026 - val_loss: 0.4164 - val_acc: 0.8784\n",
      "Epoch 121/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9005\n",
      "Epoch 00121: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2817 - acc: 0.9005 - val_loss: 0.4109 - val_acc: 0.8749\n",
      "Epoch 122/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9018\n",
      "Epoch 00122: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2796 - acc: 0.9017 - val_loss: 0.4711 - val_acc: 0.8652\n",
      "Epoch 123/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9014\n",
      "Epoch 00123: val_acc did not improve from 0.88230\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2821 - acc: 0.9015 - val_loss: 0.4497 - val_acc: 0.8705\n",
      "Epoch 124/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9020\n",
      "Epoch 00124: val_acc improved from 0.88230 to 0.88430, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2786 - acc: 0.9020 - val_loss: 0.4022 - val_acc: 0.8843\n",
      "Epoch 125/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.9006\n",
      "Epoch 00125: val_acc did not improve from 0.88430\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2833 - acc: 0.9006 - val_loss: 0.4423 - val_acc: 0.8730\n",
      "Epoch 126/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9012\n",
      "Epoch 00126: val_acc did not improve from 0.88430\n",
      "391/390 [==============================] - 37s 96ms/step - loss: 0.2792 - acc: 0.9013 - val_loss: 0.4297 - val_acc: 0.8787\n",
      "Epoch 127/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9029\n",
      "Epoch 00127: val_acc did not improve from 0.88430\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2746 - acc: 0.9029 - val_loss: 0.5026 - val_acc: 0.8655\n",
      "Epoch 128/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9028\n",
      "Epoch 00128: val_acc improved from 0.88430 to 0.89130, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.2787 - acc: 0.9028 - val_loss: 0.3716 - val_acc: 0.8913\n",
      "Epoch 129/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9032\n",
      "Epoch 00129: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 96ms/step - loss: 0.2745 - acc: 0.9031 - val_loss: 0.5972 - val_acc: 0.8370\n",
      "Epoch 130/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9026\n",
      "Epoch 00130: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.2750 - acc: 0.9026 - val_loss: 0.4157 - val_acc: 0.8780\n",
      "Epoch 131/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9039- ETA: 2\n",
      "Epoch 00131: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.2760 - acc: 0.9039 - val_loss: 0.3995 - val_acc: 0.8877\n",
      "Epoch 132/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9042\n",
      "Epoch 00132: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.2670 - acc: 0.9043 - val_loss: 0.4830 - val_acc: 0.8720\n",
      "Epoch 133/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9042\n",
      "Epoch 00133: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.2719 - acc: 0.9042 - val_loss: 0.4205 - val_acc: 0.8813\n",
      "Epoch 134/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9051\n",
      "Epoch 00134: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.2711 - acc: 0.9050 - val_loss: 0.4545 - val_acc: 0.8716\n",
      "Epoch 135/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9054\n",
      "Epoch 00135: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 94ms/step - loss: 0.2692 - acc: 0.9055 - val_loss: 0.4042 - val_acc: 0.8860\n",
      "Epoch 136/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9041\n",
      "Epoch 00136: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.2709 - acc: 0.9041 - val_loss: 0.4557 - val_acc: 0.8721\n",
      "Epoch 137/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9069\n",
      "Epoch 00137: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 37s 95ms/step - loss: 0.2681 - acc: 0.9070 - val_loss: 0.4223 - val_acc: 0.8761\n",
      "Epoch 138/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9067\n",
      "Epoch 00138: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2702 - acc: 0.9067 - val_loss: 0.4789 - val_acc: 0.8667\n",
      "Epoch 139/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9049\n",
      "Epoch 00139: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2698 - acc: 0.9048 - val_loss: 0.4986 - val_acc: 0.8599\n",
      "Epoch 140/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9075\n",
      "Epoch 00140: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2635 - acc: 0.9076 - val_loss: 0.4867 - val_acc: 0.8668\n",
      "Epoch 141/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9047\n",
      "Epoch 00141: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2657 - acc: 0.9048 - val_loss: 0.4971 - val_acc: 0.8655\n",
      "Epoch 142/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9052\n",
      "Epoch 00142: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2695 - acc: 0.9051 - val_loss: 0.5357 - val_acc: 0.8543\n",
      "Epoch 143/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9063\n",
      "Epoch 00143: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2615 - acc: 0.9062 - val_loss: 0.3795 - val_acc: 0.8882\n",
      "Epoch 144/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9080\n",
      "Epoch 00144: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2619 - acc: 0.9079 - val_loss: 0.5395 - val_acc: 0.8429\n",
      "Epoch 145/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9094\n",
      "Epoch 00145: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2598 - acc: 0.9094 - val_loss: 0.4996 - val_acc: 0.8616\n",
      "Epoch 146/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9096\n",
      "Epoch 00146: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2604 - acc: 0.9094 - val_loss: 0.4093 - val_acc: 0.8859\n",
      "Epoch 147/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9071\n",
      "Epoch 00147: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2608 - acc: 0.9071 - val_loss: 0.5897 - val_acc: 0.8461\n",
      "Epoch 148/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9058\n",
      "Epoch 00148: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2642 - acc: 0.9057 - val_loss: 0.4006 - val_acc: 0.8822\n",
      "Epoch 149/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9090\n",
      "Epoch 00149: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2589 - acc: 0.9091 - val_loss: 0.3849 - val_acc: 0.8897\n",
      "Epoch 150/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.9093- ETA: 1s - loss: 0.\n",
      "Epoch 00150: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2579 - acc: 0.9093 - val_loss: 0.4113 - val_acc: 0.8867\n",
      "Epoch 151/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9084\n",
      "Epoch 00151: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2597 - acc: 0.9084 - val_loss: 0.5415 - val_acc: 0.8529\n",
      "Epoch 152/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9080\n",
      "Epoch 00152: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2590 - acc: 0.9081 - val_loss: 0.4159 - val_acc: 0.8788\n",
      "Epoch 153/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9088\n",
      "Epoch 00153: val_acc did not improve from 0.89130\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2580 - acc: 0.9088 - val_loss: 0.3920 - val_acc: 0.8870\n",
      "Epoch 154/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9086- ETA: 2s - los - ETA: 0s - loss: 0.2581 - acc\n",
      "Epoch 00154: val_acc improved from 0.89130 to 0.89160, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2588 - acc: 0.9085 - val_loss: 0.3765 - val_acc: 0.8916\n",
      "Epoch 155/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9091\n",
      "Epoch 00155: val_acc did not improve from 0.89160\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2573 - acc: 0.9091 - val_loss: 0.3909 - val_acc: 0.8870\n",
      "Epoch 156/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9102\n",
      "Epoch 00156: val_acc improved from 0.89160 to 0.89990, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2574 - acc: 0.9103 - val_loss: 0.3416 - val_acc: 0.8999\n",
      "Epoch 157/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9120\n",
      "Epoch 00157: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2531 - acc: 0.9120 - val_loss: 0.4498 - val_acc: 0.8765\n",
      "Epoch 158/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9083\n",
      "Epoch 00158: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2587 - acc: 0.9083 - val_loss: 0.3478 - val_acc: 0.8996\n",
      "Epoch 159/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2511 - acc: 0.9110\n",
      "Epoch 00159: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2512 - acc: 0.9109 - val_loss: 0.5047 - val_acc: 0.8606\n",
      "Epoch 160/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9106\n",
      "Epoch 00160: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2541 - acc: 0.9106 - val_loss: 0.4171 - val_acc: 0.8824\n",
      "Epoch 161/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9127\n",
      "Epoch 00161: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2517 - acc: 0.9126 - val_loss: 0.7756 - val_acc: 0.8023\n",
      "Epoch 162/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9118\n",
      "Epoch 00162: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2518 - acc: 0.9118 - val_loss: 0.4007 - val_acc: 0.8887\n",
      "Epoch 163/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9106\n",
      "Epoch 00163: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2561 - acc: 0.9107 - val_loss: 0.4053 - val_acc: 0.8859\n",
      "Epoch 164/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9102\n",
      "Epoch 00164: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2533 - acc: 0.9103 - val_loss: 0.4923 - val_acc: 0.8663\n",
      "Epoch 165/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9130\n",
      "Epoch 00165: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2474 - acc: 0.9130 - val_loss: 0.3745 - val_acc: 0.8893\n",
      "Epoch 166/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9119\n",
      "Epoch 00166: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2493 - acc: 0.9119 - val_loss: 0.4049 - val_acc: 0.8833\n",
      "Epoch 167/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9108\n",
      "Epoch 00167: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2505 - acc: 0.9108 - val_loss: 0.4207 - val_acc: 0.8826\n",
      "Epoch 168/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9128\n",
      "Epoch 00168: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2506 - acc: 0.9127 - val_loss: 0.3767 - val_acc: 0.8906\n",
      "Epoch 169/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9111\n",
      "Epoch 00169: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2521 - acc: 0.9111 - val_loss: 0.4144 - val_acc: 0.8801\n",
      "Epoch 170/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9114\n",
      "Epoch 00170: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2526 - acc: 0.9114 - val_loss: 0.3516 - val_acc: 0.8947\n",
      "Epoch 171/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9140\n",
      "Epoch 00171: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2454 - acc: 0.9140 - val_loss: 0.5599 - val_acc: 0.8460\n",
      "Epoch 172/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9130\n",
      "Epoch 00172: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2470 - acc: 0.9130 - val_loss: 0.4110 - val_acc: 0.8836\n",
      "Epoch 173/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9121\n",
      "Epoch 00173: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2495 - acc: 0.9120 - val_loss: 0.3983 - val_acc: 0.8839\n",
      "Epoch 174/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9144\n",
      "Epoch 00174: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2421 - acc: 0.9142 - val_loss: 0.6538 - val_acc: 0.8368\n",
      "Epoch 175/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9119\n",
      "Epoch 00175: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2491 - acc: 0.9120 - val_loss: 0.3894 - val_acc: 0.8883\n",
      "Epoch 176/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9138\n",
      "Epoch 00176: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2449 - acc: 0.9138 - val_loss: 0.4652 - val_acc: 0.8698\n",
      "Epoch 177/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9150\n",
      "Epoch 00177: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2431 - acc: 0.9151 - val_loss: 0.4605 - val_acc: 0.8768\n",
      "Epoch 178/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9121\n",
      "Epoch 00178: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2474 - acc: 0.9121 - val_loss: 0.4396 - val_acc: 0.8752\n",
      "Epoch 179/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9128\n",
      "Epoch 00179: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2465 - acc: 0.9128 - val_loss: 0.4470 - val_acc: 0.8744\n",
      "Epoch 180/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9138\n",
      "Epoch 00180: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2445 - acc: 0.9138 - val_loss: 0.4882 - val_acc: 0.8706\n",
      "Epoch 181/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9139- ETA: 1s - loss: 0.2\n",
      "Epoch 00181: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2445 - acc: 0.9139 - val_loss: 0.3979 - val_acc: 0.8843\n",
      "Epoch 182/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9145\n",
      "Epoch 00182: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2438 - acc: 0.9145 - val_loss: 0.4263 - val_acc: 0.8835\n",
      "Epoch 183/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9125\n",
      "Epoch 00183: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2461 - acc: 0.9124 - val_loss: 0.5067 - val_acc: 0.8616\n",
      "Epoch 184/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9157\n",
      "Epoch 00184: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2423 - acc: 0.9158 - val_loss: 0.4438 - val_acc: 0.8773\n",
      "Epoch 185/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9156\n",
      "Epoch 00185: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2424 - acc: 0.9156 - val_loss: 0.3946 - val_acc: 0.8898\n",
      "Epoch 186/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9148\n",
      "Epoch 00186: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2402 - acc: 0.9148 - val_loss: 0.4895 - val_acc: 0.8731\n",
      "Epoch 187/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9141\n",
      "Epoch 00187: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2423 - acc: 0.9142 - val_loss: 0.3874 - val_acc: 0.8869\n",
      "Epoch 188/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9164\n",
      "Epoch 00188: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2383 - acc: 0.9165 - val_loss: 0.5063 - val_acc: 0.8618\n",
      "Epoch 189/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9149\n",
      "Epoch 00189: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2421 - acc: 0.9150 - val_loss: 0.4289 - val_acc: 0.8775\n",
      "Epoch 190/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9154\n",
      "Epoch 00190: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2405 - acc: 0.9155 - val_loss: 0.5525 - val_acc: 0.8469\n",
      "Epoch 191/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9162\n",
      "Epoch 00191: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2358 - acc: 0.9163 - val_loss: 0.4534 - val_acc: 0.8824\n",
      "Epoch 192/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9154- ETA: 4s - loss: 0.2388\n",
      "Epoch 00192: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2393 - acc: 0.9154 - val_loss: 0.4709 - val_acc: 0.8748\n",
      "Epoch 193/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9164\n",
      "Epoch 00193: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2358 - acc: 0.9164 - val_loss: 0.4753 - val_acc: 0.8700\n",
      "Epoch 194/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9152\n",
      "Epoch 00194: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2392 - acc: 0.9151 - val_loss: 0.4101 - val_acc: 0.8807\n",
      "Epoch 195/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9160\n",
      "Epoch 00195: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2362 - acc: 0.9161 - val_loss: 0.3740 - val_acc: 0.8909\n",
      "Epoch 196/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9170\n",
      "Epoch 00196: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2365 - acc: 0.9170 - val_loss: 0.4817 - val_acc: 0.8699\n",
      "Epoch 197/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9169\n",
      "Epoch 00197: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2379 - acc: 0.9168 - val_loss: 0.5141 - val_acc: 0.8626\n",
      "Epoch 198/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9178\n",
      "Epoch 00198: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2315 - acc: 0.9177 - val_loss: 0.4293 - val_acc: 0.8818\n",
      "Epoch 199/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9176\n",
      "Epoch 00199: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2324 - acc: 0.9177 - val_loss: 0.4413 - val_acc: 0.8822\n",
      "Epoch 200/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9166\n",
      "Epoch 00200: val_acc did not improve from 0.89990\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2338 - acc: 0.9166 - val_loss: 0.4441 - val_acc: 0.8768\n",
      "Epoch 201/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9184\n",
      "Epoch 00201: val_acc improved from 0.89990 to 0.90240, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2323 - acc: 0.9184 - val_loss: 0.3344 - val_acc: 0.9024\n",
      "Epoch 202/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9187\n",
      "Epoch 00202: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2327 - acc: 0.9187 - val_loss: 0.4380 - val_acc: 0.8834\n",
      "Epoch 203/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9189\n",
      "Epoch 00203: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2314 - acc: 0.9189 - val_loss: 0.3648 - val_acc: 0.8958\n",
      "Epoch 204/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9160\n",
      "Epoch 00204: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2366 - acc: 0.9160 - val_loss: 0.4576 - val_acc: 0.8743\n",
      "Epoch 205/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9168\n",
      "Epoch 00205: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2347 - acc: 0.9167 - val_loss: 0.3815 - val_acc: 0.8922\n",
      "Epoch 206/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9188- ETA: 2s - los\n",
      "Epoch 00206: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2336 - acc: 0.9187 - val_loss: 0.4275 - val_acc: 0.8775\n",
      "Epoch 207/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9191- ETA: 0s - loss: 0.2305 - acc: 0\n",
      "Epoch 00207: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2302 - acc: 0.9190 - val_loss: 0.4250 - val_acc: 0.8828\n",
      "Epoch 208/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9181\n",
      "Epoch 00208: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2323 - acc: 0.9181 - val_loss: 0.4312 - val_acc: 0.8795\n",
      "Epoch 209/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9187\n",
      "Epoch 00209: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2318 - acc: 0.9187 - val_loss: 0.3660 - val_acc: 0.8982\n",
      "Epoch 210/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9175\n",
      "Epoch 00210: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2316 - acc: 0.9174 - val_loss: 0.4273 - val_acc: 0.8825\n",
      "Epoch 211/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9178\n",
      "Epoch 00211: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2299 - acc: 0.9178 - val_loss: 0.4319 - val_acc: 0.8786\n",
      "Epoch 212/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9183\n",
      "Epoch 00212: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2268 - acc: 0.9183 - val_loss: 0.4200 - val_acc: 0.8836\n",
      "Epoch 213/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9191\n",
      "Epoch 00213: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2289 - acc: 0.9190 - val_loss: 0.3964 - val_acc: 0.8857\n",
      "Epoch 214/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9189\n",
      "Epoch 00214: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2298 - acc: 0.9189 - val_loss: 0.3639 - val_acc: 0.8948\n",
      "Epoch 215/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9191- ETA: 2s - loss\n",
      "Epoch 00215: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2296 - acc: 0.9191 - val_loss: 0.3814 - val_acc: 0.8947\n",
      "Epoch 216/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9187\n",
      "Epoch 00216: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2280 - acc: 0.9188 - val_loss: 0.3481 - val_acc: 0.9002\n",
      "Epoch 217/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9216\n",
      "Epoch 00217: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2231 - acc: 0.9215 - val_loss: 0.4287 - val_acc: 0.8815\n",
      "Epoch 218/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9198\n",
      "Epoch 00218: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2302 - acc: 0.9199 - val_loss: 0.3792 - val_acc: 0.8973\n",
      "Epoch 219/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9212\n",
      "Epoch 00219: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2258 - acc: 0.9212 - val_loss: 0.4492 - val_acc: 0.8756\n",
      "Epoch 220/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9187\n",
      "Epoch 00220: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2278 - acc: 0.9187 - val_loss: 0.3677 - val_acc: 0.8983\n",
      "Epoch 221/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9203\n",
      "Epoch 00221: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2268 - acc: 0.9203 - val_loss: 0.4079 - val_acc: 0.8835\n",
      "Epoch 222/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9207\n",
      "Epoch 00222: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2262 - acc: 0.9208 - val_loss: 0.3796 - val_acc: 0.8937\n",
      "Epoch 223/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9196\n",
      "Epoch 00223: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2276 - acc: 0.9196 - val_loss: 0.4039 - val_acc: 0.8916\n",
      "Epoch 224/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9193\n",
      "Epoch 00224: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2258 - acc: 0.9194 - val_loss: 0.4462 - val_acc: 0.8819\n",
      "Epoch 225/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9184\n",
      "Epoch 00225: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2275 - acc: 0.9185 - val_loss: 0.4177 - val_acc: 0.8850\n",
      "Epoch 226/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9203\n",
      "Epoch 00226: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2238 - acc: 0.9201 - val_loss: 0.4311 - val_acc: 0.8828\n",
      "Epoch 227/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9212- ETA: 1s - loss: 0.2\n",
      "Epoch 00227: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2220 - acc: 0.9212 - val_loss: 0.3986 - val_acc: 0.8908\n",
      "Epoch 228/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9200\n",
      "Epoch 00228: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2215 - acc: 0.9200 - val_loss: 0.4110 - val_acc: 0.8819\n",
      "Epoch 229/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9208\n",
      "Epoch 00229: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2220 - acc: 0.9208 - val_loss: 0.3757 - val_acc: 0.8918\n",
      "Epoch 230/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9206\n",
      "Epoch 00230: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2252 - acc: 0.9206 - val_loss: 0.4769 - val_acc: 0.8718\n",
      "Epoch 231/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9225\n",
      "Epoch 00231: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2201 - acc: 0.9226 - val_loss: 0.3525 - val_acc: 0.8980\n",
      "Epoch 232/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9236\n",
      "Epoch 00232: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2159 - acc: 0.9236 - val_loss: 0.4565 - val_acc: 0.8787\n",
      "Epoch 233/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9213\n",
      "Epoch 00233: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2218 - acc: 0.9213 - val_loss: 0.4153 - val_acc: 0.8857\n",
      "Epoch 234/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9219- ETA: 2s - lo\n",
      "Epoch 00234: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2206 - acc: 0.9219 - val_loss: 0.3585 - val_acc: 0.8987\n",
      "Epoch 235/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9207\n",
      "Epoch 00235: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2207 - acc: 0.9206 - val_loss: 0.3869 - val_acc: 0.8912\n",
      "Epoch 236/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9220\n",
      "Epoch 00236: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2187 - acc: 0.9220 - val_loss: 0.4815 - val_acc: 0.8723\n",
      "Epoch 237/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9229\n",
      "Epoch 00237: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2174 - acc: 0.9229 - val_loss: 0.3632 - val_acc: 0.8988\n",
      "Epoch 238/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9223\n",
      "Epoch 00238: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2203 - acc: 0.9223 - val_loss: 0.4030 - val_acc: 0.8903\n",
      "Epoch 239/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9234\n",
      "Epoch 00239: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 101ms/step - loss: 0.2140 - acc: 0.9233 - val_loss: 0.4148 - val_acc: 0.8870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9220\n",
      "Epoch 00240: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2237 - acc: 0.9219 - val_loss: 0.5174 - val_acc: 0.8624\n",
      "Epoch 241/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9215\n",
      "Epoch 00241: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2208 - acc: 0.9215 - val_loss: 0.4166 - val_acc: 0.8874\n",
      "Epoch 242/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9249\n",
      "Epoch 00242: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2146 - acc: 0.9248 - val_loss: 0.3950 - val_acc: 0.8875\n",
      "Epoch 243/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9229\n",
      "Epoch 00243: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2166 - acc: 0.9229 - val_loss: 0.4983 - val_acc: 0.8665\n",
      "Epoch 244/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9235\n",
      "Epoch 00244: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2171 - acc: 0.9236 - val_loss: 0.4743 - val_acc: 0.8763\n",
      "Epoch 245/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9228\n",
      "Epoch 00245: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2160 - acc: 0.9229 - val_loss: 0.3576 - val_acc: 0.8996\n",
      "Epoch 246/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9240\n",
      "Epoch 00246: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2146 - acc: 0.9240 - val_loss: 0.3925 - val_acc: 0.8912\n",
      "Epoch 247/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9244\n",
      "Epoch 00247: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 98ms/step - loss: 0.2134 - acc: 0.9243 - val_loss: 0.3922 - val_acc: 0.8918\n",
      "Epoch 248/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9245\n",
      "Epoch 00248: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2133 - acc: 0.9245 - val_loss: 0.4253 - val_acc: 0.8844\n",
      "Epoch 249/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9247\n",
      "Epoch 00249: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2113 - acc: 0.9247 - val_loss: 0.3889 - val_acc: 0.8941\n",
      "Epoch 250/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9259\n",
      "Epoch 00250: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2111 - acc: 0.9259 - val_loss: 0.3759 - val_acc: 0.8974\n",
      "Epoch 251/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9237\n",
      "Epoch 00251: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2170 - acc: 0.9236 - val_loss: 0.4283 - val_acc: 0.8837\n",
      "Epoch 252/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9255\n",
      "Epoch 00252: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2141 - acc: 0.9256 - val_loss: 0.4018 - val_acc: 0.8924\n",
      "Epoch 253/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9238\n",
      "Epoch 00253: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2151 - acc: 0.9238 - val_loss: 0.4357 - val_acc: 0.8860\n",
      "Epoch 254/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9248\n",
      "Epoch 00254: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2123 - acc: 0.9248 - val_loss: 0.3893 - val_acc: 0.8878\n",
      "Epoch 255/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9237\n",
      "Epoch 00255: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2178 - acc: 0.9237 - val_loss: 0.4824 - val_acc: 0.8732\n",
      "Epoch 256/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9251\n",
      "Epoch 00256: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2149 - acc: 0.9252 - val_loss: 0.4245 - val_acc: 0.8822\n",
      "Epoch 257/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9243\n",
      "Epoch 00257: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2146 - acc: 0.9244 - val_loss: 0.3907 - val_acc: 0.8932\n",
      "Epoch 258/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9249- ETA: 1s - loss: 0.2132 - \n",
      "Epoch 00258: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2130 - acc: 0.9249 - val_loss: 0.4708 - val_acc: 0.8738\n",
      "Epoch 259/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9234\n",
      "Epoch 00259: val_acc did not improve from 0.90240\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2176 - acc: 0.9233 - val_loss: 0.4535 - val_acc: 0.8791\n",
      "Epoch 260/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9246\n",
      "Epoch 00260: val_acc improved from 0.90240 to 0.90800, saving model to DNST_model_with_augmentation.hdf5\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2140 - acc: 0.9246 - val_loss: 0.3285 - val_acc: 0.9080\n",
      "Epoch 261/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9251\n",
      "Epoch 00261: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2149 - acc: 0.9251 - val_loss: 0.4503 - val_acc: 0.8816\n",
      "Epoch 262/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9248\n",
      "Epoch 00262: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2136 - acc: 0.9247 - val_loss: 0.3756 - val_acc: 0.8977\n",
      "Epoch 263/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9246\n",
      "Epoch 00263: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2130 - acc: 0.9247 - val_loss: 0.4343 - val_acc: 0.8821\n",
      "Epoch 264/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9246\n",
      "Epoch 00264: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 96ms/step - loss: 0.2141 - acc: 0.9247 - val_loss: 0.4362 - val_acc: 0.8793\n",
      "Epoch 265/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9265\n",
      "Epoch 00265: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2118 - acc: 0.9265 - val_loss: 0.3787 - val_acc: 0.8939\n",
      "Epoch 266/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9257- ETA: 6s\n",
      "Epoch 00266: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2108 - acc: 0.9257 - val_loss: 0.4103 - val_acc: 0.8910\n",
      "Epoch 267/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9256\n",
      "Epoch 00267: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2084 - acc: 0.9255 - val_loss: 0.3984 - val_acc: 0.8910\n",
      "Epoch 268/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9256-\n",
      "Epoch 00268: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2107 - acc: 0.9255 - val_loss: 0.3941 - val_acc: 0.8933\n",
      "Epoch 269/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9260\n",
      "Epoch 00269: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2071 - acc: 0.9260 - val_loss: 0.3542 - val_acc: 0.9032\n",
      "Epoch 270/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9263\n",
      "Epoch 00270: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 37s 96ms/step - loss: 0.2080 - acc: 0.9263 - val_loss: 0.3834 - val_acc: 0.8934\n",
      "Epoch 271/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9277\n",
      "Epoch 00271: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2049 - acc: 0.9277 - val_loss: 0.4722 - val_acc: 0.8784\n",
      "Epoch 272/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9259\n",
      "Epoch 00272: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2099 - acc: 0.9258 - val_loss: 0.4187 - val_acc: 0.8879\n",
      "Epoch 273/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9271\n",
      "Epoch 00273: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2068 - acc: 0.9271 - val_loss: 0.4135 - val_acc: 0.8882\n",
      "Epoch 274/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9266\n",
      "Epoch 00274: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 37s 96ms/step - loss: 0.2063 - acc: 0.9266 - val_loss: 0.3961 - val_acc: 0.8934\n",
      "Epoch 275/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9261\n",
      "Epoch 00275: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2104 - acc: 0.9261 - val_loss: 0.4087 - val_acc: 0.8911\n",
      "Epoch 276/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9254\n",
      "Epoch 00276: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2096 - acc: 0.9254 - val_loss: 0.3625 - val_acc: 0.8949\n",
      "Epoch 277/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9263\n",
      "Epoch 00277: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2073 - acc: 0.9263 - val_loss: 0.3852 - val_acc: 0.8972\n",
      "Epoch 278/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9244\n",
      "Epoch 00278: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2124 - acc: 0.9244 - val_loss: 0.4904 - val_acc: 0.8720\n",
      "Epoch 279/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9271\n",
      "Epoch 00279: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2079 - acc: 0.9271 - val_loss: 0.4147 - val_acc: 0.8874\n",
      "Epoch 280/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9257\n",
      "Epoch 00280: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2082 - acc: 0.9257 - val_loss: 0.3974 - val_acc: 0.8931\n",
      "Epoch 281/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9262\n",
      "Epoch 00281: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2076 - acc: 0.9261 - val_loss: 0.3598 - val_acc: 0.8999\n",
      "Epoch 282/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9281\n",
      "Epoch 00282: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2057 - acc: 0.9280 - val_loss: 0.4034 - val_acc: 0.8921\n",
      "Epoch 283/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9266\n",
      "Epoch 00283: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2076 - acc: 0.9266 - val_loss: 0.4033 - val_acc: 0.8926\n",
      "Epoch 284/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9264\n",
      "Epoch 00284: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2076 - acc: 0.9264 - val_loss: 0.4590 - val_acc: 0.8797\n",
      "Epoch 285/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9274\n",
      "Epoch 00285: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2072 - acc: 0.9274 - val_loss: 0.3904 - val_acc: 0.8933\n",
      "Epoch 286/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9270\n",
      "Epoch 00286: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2083 - acc: 0.9269 - val_loss: 0.3932 - val_acc: 0.8950\n",
      "Epoch 287/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9272\n",
      "Epoch 00287: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2061 - acc: 0.9271 - val_loss: 0.5821 - val_acc: 0.8488\n",
      "Epoch 288/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9283\n",
      "Epoch 00288: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2016 - acc: 0.9283 - val_loss: 0.3989 - val_acc: 0.8867\n",
      "Epoch 289/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9274- ETA: 2s - \n",
      "Epoch 00289: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2068 - acc: 0.9274 - val_loss: 0.4030 - val_acc: 0.8943\n",
      "Epoch 290/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9276\n",
      "Epoch 00290: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2041 - acc: 0.9276 - val_loss: 0.3854 - val_acc: 0.8996\n",
      "Epoch 291/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9282\n",
      "Epoch 00291: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 97ms/step - loss: 0.2027 - acc: 0.9282 - val_loss: 0.5111 - val_acc: 0.8694\n",
      "Epoch 292/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9276\n",
      "Epoch 00292: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 98ms/step - loss: 0.2034 - acc: 0.9276 - val_loss: 0.3579 - val_acc: 0.9023\n",
      "Epoch 293/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9273- ETA: 1s - loss: 0.201\n",
      "Epoch 00293: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2021 - acc: 0.9272 - val_loss: 0.3475 - val_acc: 0.9040\n",
      "Epoch 294/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9283\n",
      "Epoch 00294: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 38s 98ms/step - loss: 0.2047 - acc: 0.9283 - val_loss: 0.4033 - val_acc: 0.8932\n",
      "Epoch 295/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9275\n",
      "Epoch 00295: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2060 - acc: 0.9275 - val_loss: 0.3833 - val_acc: 0.8958\n",
      "Epoch 296/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9278\n",
      "Epoch 00296: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2021 - acc: 0.9277 - val_loss: 0.4062 - val_acc: 0.8908\n",
      "Epoch 297/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9287\n",
      "Epoch 00297: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2031 - acc: 0.9286 - val_loss: 0.5378 - val_acc: 0.8627\n",
      "Epoch 298/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9282\n",
      "Epoch 00298: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2001 - acc: 0.9284 - val_loss: 0.3998 - val_acc: 0.8904\n",
      "Epoch 299/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9263\n",
      "Epoch 00299: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 100ms/step - loss: 0.2081 - acc: 0.9263 - val_loss: 0.3827 - val_acc: 0.8952\n",
      "Epoch 300/300\n",
      "390/390 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9272\n",
      "Epoch 00300: val_acc did not improve from 0.90800\n",
      "391/390 [==============================] - 39s 99ms/step - loss: 0.2033 - acc: 0.9272 - val_loss: 0.3999 - val_acc: 0.8922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1888d7f4518>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 300\n",
    "batch_size=128\n",
    "#datagen.flow(x_train, y_train, batch_size=128)\n",
    "model1.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(X_train) / batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, y_test),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "a0345aa5-79ff-4e56-eb94-50437b43c4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 262us/sample - loss: 0.3993 - acc: 0.8922\n",
      "Test loss: 0.3993266311496496\n",
      "Test accuracy: 0.8922\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model1.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UE3lF6EH1r_L",
    "outputId": "92df862c-76a7-4a02-9533-6c164bc5984d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model1.save_weights(\"DNST_model_with_augmentation.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Internshala Assignment DNST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
